{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOIBvKGH3LA8UZtyg9o3ok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeCodingStudio/Python_NLP/blob/master/4_%EC%9B%8C%EB%93%9C_%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 워드 임베딩(Word Embedding)\n",
        "* 단어를 컴퓨터가 이해하고, 효율적으로 처리할 수 있도록 단어를 벡터화하는 기술\n",
        "* 단어를 밀집 벡터의 형태로 표현하는 방법\n",
        "* 워드 임베딩 과정을 통해 나온 결과를 임베딩 벡터\n",
        "* 워드 임베딩을 거쳐 잘 표현된 단어 벡터들은 계산이 가능하며, 모델에 입력으로 사용할 수 있음"
      ],
      "metadata": {
        "id": "HLwi5x4cGnK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 인코딩(Encoding)\n",
        "* 기계는 자연어를 이해할 수 없기 때문에 데이터를 기계가 이해할 수 있도록 숫자 등으로 변환해주는 작업\n",
        "* 자연어를 수치화된 벡터로 변환하는 작업"
      ],
      "metadata": {
        "id": "RxxYA-zFHvBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. 희소 표현(Sparse Representation)\n",
        "* 원-핫 인코딩을 통해서 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법\n",
        "* 벡터 또는 행렬의 값이 대부분이 0으로 표현되는 방법을 희소 표현이라고 함\n",
        "* 원-핫 인코딩에 의해 만들어지는 벡터를 희소 벡터라고 함"
      ],
      "metadata": {
        "id": "IAHriwavIbsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. 희소 벡터의 문제점\n",
        "* 희소 벡터의 특징은 단어의 개수가 늘어나면 벡터의 차원이 한없이 커진다는 것\n",
        "* 원-핫 벡터는 벡터 표현 방식이 매우 단순하여, 단순히 단어의 출현 여부만을 벡터에 표시할 수 있음\n",
        "* 희소 벡터를 이용하여 문장 또는 텍스트 간의 유사도를 계산해보면 원하는 유사도를 얻기 힘듬"
      ],
      "metadata": {
        "id": "jqyt-g1LLGMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-4. 밀집 표현(Dense Represenstation)\n",
        "* 벡터의 차원이 조밀해졌다는 의미\n",
        "* 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞추는 표현 방식\n",
        "* 자연어를 밀집 표현으로 변환하는 인코딩 과정에서 0과 1의 binary 값이 아니라 연속적인 실수 값을 가질 수 있음"
      ],
      "metadata": {
        "id": "uoPkXWJwLfDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-5. 밀집 표현의 장점\n",
        "* 적은 차원으로 대상을 표현할 수 있음\n",
        "* 더 큰 일반화 능력을 가지고 있음"
      ],
      "metadata": {
        "id": "U3ZwNuu9L9mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-6. 원-핫 벡터와 워드 인베딩의 차이\n",
        "\n",
        "|종류|원-핫 백터|워드 임베딩|\n",
        "|------|---|---|\n",
        "|차원|고차원(단어의 집합의 크기)|저차원으로 표현 가능|\n",
        "|종류|희소 백터|밀집 백터|\n",
        "|표현방법|수동|학습 코퍼스에서 학습|\n",
        "|값의 유형|0,1|실수|"
      ],
      "metadata": {
        "id": "3Gsy9detMyoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-7. 차원 축소(Dimensionality Reduction)\n",
        "* 희소 벡터를 밀집 벡터의 형태로 변환하는 방법\n",
        "* 머신러닝에서 많은 피터들로 구성된 고차원의 데이터에서 중요한 피처들만 뽑아 저차원의 데이터(행렬)로 변환하기 위해 사용\n",
        "    - PCA(Principal Component Analysis)\n",
        "    - 잠재 의미 분석(Latent Semantic Analysis)\n",
        "    - 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)\n",
        "    - SVD(Sigular Value Decomposition, SVD)"
      ],
      "metadata": {
        "id": "Da0S6WpOOlpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Word2Vec"
      ],
      "metadata": {
        "id": "bPL0j49nS3sN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. 분산 표현(Distributed Representation)\n",
        "* 분포 가설이라는 가정 하에 만들어진 표현 방법\n",
        "* 분포 가설: \"비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다\"는 가설\n",
        "* 분포 가설의 목표는 단어 주변의 단어들, window 크기에 따라 정의되는 문맥의 의미를 이용해 단어를 벡터로 표현(분산 표현) 하는 것\n",
        "* 분산 표현으로 표현된 벡터들은 원-핫 벡터처럼 차원이 단어 집합의 크기일 필요가 없으므로, 벡터의 차원이 상대적으로 저차원으로 줄어듬\n",
        "* 밀집 표현을 분산 표현이라 부르기도 함\n",
        "* 희소 표현에서는 각각의 차원이 각각의 독립적인 정보를 갖고 있지만, 밀집에서는 하나의 차원이 여러 속성들이 버무려진 정보를 갖고 있음\n",
        "* 밀집 표현을 이용한 대표적인 학습 방법이 WOrd2Vec 임"
      ],
      "metadata": {
        "id": "tGavNbyrTmvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2. Word2Vec이란?\n",
        "* 분포 가설 하에 표현한 분산 표현을 따르는 워드 임베딩 모델\n",
        "* Google이 2013년도 처음 공개\n",
        "* 중심 단어와 주변의 단어들을 사용하여 단어를 예측하는 방식으로 임베딩을 만듬\n",
        "* WOrd2Vec의 학습 방식에는 두가지 방식\n",
        "    - CBOW(Continuous Bag of Words)\n",
        "    - Skip-gram"
      ],
      "metadata": {
        "id": "p53ZEx8DUEJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-3. CBOW(Continuous Bag of Words)\n",
        "* [Word2Vec](https://wikidocs.net/22660)\n",
        "* 주변에 있는 단어들을 보고 중간에 있는 단어를 예측하는 방법\n",
        "* 주변 단어(context)는 타겟 단어(target word)의 직전 n개 단어와 직후 n개 단어를 의미하며, 이 범위를 window라 부르고, n을 window size라고 함\n",
        "* 문장 하나에 대해 한 번만 학습을 진행하는 것은 아깝기 때문에 sliding window 방식을 사용하여 하나의 문장을 가지고 여러 개의 학습 데이터셋을 만듬\n",
        "* Word2Vec은 최초 입력으로 one-hot-vector를 받는데, 1*V 크기의 one-hot-vector의 각 요소와 hidden layer N개의 각 노드는 1대1 대응이 이워져야 하므로 가중치 행렬 W의 크기는 VXN이 됨\n",
        "* 학습 코퍼스에 단어가 10,000개 있고 hidden layer의 노드를 300개로 지정하면, 가중치 행렬 W는 10,000 * 300 행렬 형태가 됨\n",
        "* 각각의 가중치 행렬은 랜덤한 값으로 초기화되어 있고, 학습 시 target word를 맞추는 과정에서 W가 계속해서 조정됨\n",
        "* 예를들어 4개의 단어들이 target word 예측에 사용될 때 각각의 단어들에 해당하는 W의 임베딩 벡터들의 평균을 사용함\n",
        "* 평균 벡터는 두 번째 가중치 행렬 W' 와 곱해지며 곱해진 경과로는 target word의 원 핫 벡터와 크기가 동일한 벡터를 얻을 수 있음\n",
        "* 최종 출력 값 벡터는 다중 클래스 분류 문제를 위한 일종의 스코어 벡터이며 0과 1사이의 값을 가지는데 이는 중심 단어일 확률을 나타냄\n",
        "* 스코어 벡터 값은 정답 레이블에 해당하는 target word의 원-핫 벡터 내 1의 값에 가까워져야 함\n",
        "* 스코어 벡터와 원-핫 벡터의 오차를 줄이기 위해 손실 함수(cross-entropy) 함수를 사용함"
      ],
      "metadata": {
        "id": "1mAOvdo1WgBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-4. Skip-gram\n",
        "* 중심 단어에서 주변 단어를 예측\n",
        "* 중심 단어를 sliding window 하면서 학습 데이터를 증강\n",
        "* 중심 단어를 가지고 주변 단어를 예측하는 방법이기 때문에 projecttion layer에서 벡터들 간의 평균을 구하는 과정이 없으며 대신 output layer를 통해 벡터가 window size의 2n개 만큼 나옴"
      ],
      "metadata": {
        "id": "KAVwUADhXJoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-5. CBOW vs Skip-gram\n",
        "* Skip-gram이 CBOW에 비해 여러 문맥을 고려하기 때문에 Skip-gram의 성능이 일반적으로 더 좋음\n",
        "* Skip-gram이 단어 당 학습 횟수가 더 많고, 임베딩의 조정 기회가 많으므로 더 정교한 임베딩 학습이 가능\n",
        "\n",
        "```\n",
        "작고 귀여운 강아지 문 앞에 앉아 있다\n",
        "```\n",
        "\n",
        "|     | CBOW |  |\n",
        "| --- | --------- | ----------------- |\n",
        "| Input  | Output | 학습기회 |\n",
        "| 귀여운, 강아지 | 작고 | 1 |\n",
        "| 작고, 강아지, 문 | 귀여운 | 1 |\n",
        "| 작고, 귀여운, 문, 앞에 | 강아지 | 1 |\n",
        "| 귀여운, 강아지, 앞에, 앉아 | 문 | 1 |\n",
        "| 강아지, 문, 앉아, 있다 | 앞에 | 1 |\n",
        "| 앞에, 앉아 | 있다 | 1 |\n",
        "\n",
        "|     | Skip-gram |  |\n",
        "| --- | --------- | ----------------- |\n",
        "| Input  | Output | 학습기회 |\n",
        "| 작고 | 귀여운, 강아지 | 2 |\n",
        "| 귀여운 | 작고, 강아지, 문 | 3 |\n",
        "| 강아지 | 작고, 귀여운, 문, 앞에 | 4 |\n",
        "| 문 | 귀여운, 강아지, 앞에, 앉아 | 4 |\n",
        "| 앞에 | 강아지, 문, 앉아, 있다 | 4 |\n",
        "| 앉아 | 문, 앞에, 있다 | 3 |\n",
        "| 있다 | 앞에, 앉아 | 2 |"
      ],
      "metadata": {
        "id": "0nD9dTQNhZH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-6. Word2Vec의 한계점\n",
        "* 단어의 형태학적 특성을 반영하지 못함\n",
        "- 예) teach, teacher, tearchers 와 같이 세 단어는 의미적으로 유사한 단어지만 각 단어를 개별단어로 처리하여 세 단어 모두 벡터 값이 다르게 구성됨\n",
        "* 단어 빈도 수의 영향을 많이 받아 희소한 단어를 임베딩하기 어려움\n",
        "* OOV(Out of Vocabulary)의 처리가 어려움\n",
        "- 새로운 단어가 등장하면 데이터 전체를 다시 학습시켜야 함\n",
        "* 단어 사전의 크기가 클수록 학습하는데 오래거림\n",
        "- 단어 사전의 크기가 수 만개 이상인 경우, Word2Vec은 학습하기에 무거운 모델이 됨"
      ],
      "metadata": {
        "id": "YV7Ktahtkopy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-7. Word2Vec의 학습 트릭\n",
        "* Subsampling Frequent Words\n",
        "* Negative Sampling"
      ],
      "metadata": {
        "id": "hqbmLKmRld2S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uH0WSbDNmUmD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}