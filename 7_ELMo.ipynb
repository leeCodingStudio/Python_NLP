{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPArr1FWQj4zI7wgeGBkbpQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeCodingStudio/Python_NLP/blob/master/7_ELMo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. ELMo(Embeddings from Language Model)\n",
        "* 2018년에 논문에서 제안된 새로운 워드 임베딩 방법론\n",
        "* 언어 모델로 하는 임베딩을 의미\n",
        "* ELMo의 가장 큰 특징은 사전 훈련된 언어 모델(Pre-Trained Language Model)을 사용한다는 것"
      ],
      "metadata": {
        "id": "YVl_60AEvZMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 기존 워드 임베딩의 한계\n",
        "* 주변 문맥 정보를 활용하여 단어를 벡터로 표현하는 방법\n",
        "* 같은 표기의 단어를 문맥에 따라서 다르게 임베딩 할 수 없음"
      ],
      "metadata": {
        "id": "YIIXLmPcwQlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. ELMo의 특징\n",
        "* 사전 학습된 단어 표현을 사용했다는 것\n",
        "* 사전학습은 대량의 자연어 코퍼스를 미리 학습하여, 자연어 코퍼스 안에 포함된 일반화된 언어 특성들을 모델의 파라미터 안에 함축하는 방법\n",
        "* 기존 논문에서도 일반화된 언어 특성들을 고려하기 위해 사전 학습된 단어 표현들을 사용했으나 문법이나 문맥 내 의미, 문장의 다의성을 학습한 고품질의 단어 표현을 얻는 것은 어려운 일이였음\n",
        "* ELMo의 단어 표현은 문장을 구성하는 각각의 간어 토큰이 전체 입력 문장, 즉 문맥을 고려하는 정보들을 표현 내에 압축한다는 점에서 전통적인 단어 임베딩과 다름\n",
        "* ELMo의 단어 벡터는 LSTM의 각 layer에서 나온 hidden representation들을 종합한 표현"
      ],
      "metadata": {
        "id": "-hCi1KdRxawL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. biLM(bidirectional Language Model)\n",
        "* ELMo는 biLM이라는 구조를 사용\n",
        "* 기존의 Seq2Seq 같은 언어 모델에서는 학습이 한 방향으로 진행되는데, 이렇게되면 문맥을 한 방향으로만 고려하여 압축된 표현을 출력으로 반환\n",
        "* biLM은 N개의 tokens으로 이루어진 입력 문장을 양방향(forward, backward)의 언어 모델링을 통해 문맥적인 표현을 반영하여 해당 입력 문장의 확률을 예측\n",
        "* ELMo는 기본적으로 LSTM을 사용하여 biLM을 구성 이러한 biLSTM Layer들에서 나온 각각의 벡터 표현들을 선형 결합하는 방법으로 각각의 단어를 표현"
      ],
      "metadata": {
        "id": "ziH1kUHy1xdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-4. ELMo 학습 방법\n",
        "* 특정 자연어처리 작업을 수행하기 위한 가전 학습된 biLM을 목표로 함\n",
        "* 사전 학습된 biLM이 있다면, 지도학습 형태의 다운스트림 작업을 수행하기 위한 언어 모델이 있을 때, biLM을 학습하는 방법은 아래와 같음\n",
        "    - 사전학습된 biLM에 원하는 자연어처리 작업의 학습 데이터셋 문장들을 통과시켜서 문장을 구성하는 각각의 단어 토큰 k에 대한 layer representation을 계산\n",
        "    - biLM에서는 양방향 학습으로 총 L개의 layer에 대해 2L + 1대의 layer representation을 얻을 수 있으므로, 이들을 모두 계산한 후에 biLM의 모든 파라미터를 freeze\n",
        "    - ELMo를 추가한 지도학습 다운스트임 작업을 수행하는 모델을 학습하여 softmax 가중치를 얻을 수 있움\n",
        "\n",
        "<center><img src=\"https://wikidocs.net/images/page/33930/elmorepresentation.PNG\"></center>"
      ],
      "metadata": {
        "id": "NoKO6FB74dk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-5. ELMo 요약\n",
        "* ELMo는 기존의 단어 임베딩 구조가 문맥의 정보를 충분히 반영하지 모산다는 한계를 지적\n",
        "* 한계를 해결하기 위해 양방향 학습이 가능한 biLM으로부터 문맥 내 정보를 충분히 반영하는 문장 벡터 표현을 학습하는 일반적인 방법을 소개\n",
        "* 넓은 범위의 NLP 문제들에서 ELMo를 적용했을 때 많은 성능 향상을 가져옴\n",
        "* 학습데이터가 작을수록 ELMo를 사용하면 더 효율적인 학습이 가능\n"
      ],
      "metadata": {
        "id": "lXZovTnU5TN9"
      }
    }
  ]
}