{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe9ikiSs7hGfnHzGPAFcGZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeCodingStudio/Python_NLP/blob/master/6_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 문장 임베딩\n",
        "* 2017년 이전의 임베딩 기법들은 대부분 단어 수준모델이였음(Word2Vec, FastText, BloVe)\n",
        "* 단어 수준 임베딩 기법은 자연어의 특성인 모호성, 동음이의어를 구분하기 어렵다는 한계가 있음\n",
        "* 2017년 이후에는 ELMo(Embeddings from Language Models)와 같은 모델이 발표되고 트랜스포머와 같은 언어 모델에서 문장 수준의 언어 모델링을 고려하면서 한계점들이 해결됨"
      ],
      "metadata": {
        "id": "siUCVAL9fQcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 언어 모델\n",
        "* 자연어처리 작업은 자연어 문장을 생성하거나 예측하는 방식으로 결과를 표현함\n",
        "* 자연어처리 작업에서는 자연어를 수치화하여 표현할 수 있는 언어 모델을 사용\n",
        "* 언어 모델은 자연어 문장 혹은 단어에 확률을 할당하여 컴퓨터가 처리할 수 있도록 하는 모델로 주어진 입력에 대해 가장 자연스러운 단어 시퀀스를 찾을 수 있음"
      ],
      "metadata": {
        "id": "pJdLVEvsf_LK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. 언어 모델링\n",
        "* 주어진 단어들로부터 아직 모르는 단어들을 예측하는 작업\n",
        "* 사람은 수많은 단어와 문장을 듣고 쓰고 말하며 언어 능력을 학습해왔지 때문에 문장 구성 및 의미상 가장 적합한 단어를 판단할 수 있음\n",
        "* 문장을 기계에게 보여주고 적합한 단어를 예측하도록 학습(기계 역시 사람과 같은 프로세스로 동작함)\n",
        "* 언어 모델은 텍스트 기반의 수많은 문장을 통해 어떤 단어가 어떤 어순으로 쓰인 것이 가장 자연스러운 문장인지 학습함"
      ],
      "metadata": {
        "id": "VjAG2ZpDgVTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 자연어처리 모델 구조\n",
        "* 자연어처리 분야의 인공지능 모델은 근 10년 동안 수 많은 모델 구조에 걸쳐 진화해 왔음\n",
        "* 사용되지 않는 모델들도 있으나, 해당 모델이 나온 이유와 구조, 한계점들을 공부하는 것은 자연어처리 분야에서 통찰을 얻는데 도움이 될 것\n",
        "* 현재에도 분야별로 다양한 모델들이 공개되고 있는데, 자연어처리에서 핵심은 공개된 모델들 중 어떤 언어 모델이 내가 풀고자 하는 문제에 가장 적합한지 탐색하는 것\n",
        "* 대부분의 분야에서 Transformer 계열의 모델이 가장 인기있지만, 특정 자연어 작업 처리에 특화된 세부적인 테크닉들이 다르므로 최신 연구 동향과 SOTA 모델들을 팔로업하는 것이 좋음"
      ],
      "metadata": {
        "id": "dA0p2zAxhhFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. 자언여처리 분야의 주요 언어 모델\n",
        "* Seq2Seq\n",
        "* ELMo\n",
        "* Transformer\n",
        "* GPT\n",
        "* BERT"
      ],
      "metadata": {
        "id": "foJYsm6ijpBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Seq2Seq 배경\n",
        "* Seq2Seq 모델이 등장하기 전에 DNN(Deep Neural Network) 모델은 사물 인식, 음성 인식 등에서 꾸준히 성과를 냈음 (예: CNN, RNN, LSTM, GRU..)\n",
        "* 모델 입/출력의 크기가 고정된다는 한계점이 존재했기 때문에 자연어처리와 같은 가변적인 길이의 입/출력을 처리하는 문제들을 제대로 해결할 수 없었음\n",
        "* [RNN](https://ardino-lab.com/rnn%EC%9D%98-%EA%B5%AC%EC%A1%B0-%EB%B0%8F-%ED%95%9C%EA%B3%84/)은 Seq2Seq가 등장하기 전에 입/출력을 시퀀스 단위로 처리할 수 있는 모델이었음\n",
        "* RNN은 셀을 재귀적으로 활용하여 연속된 입/출력을 처리할 수 있는 모델"
      ],
      "metadata": {
        "id": "_1BiJNHAk-Ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1. Seq2Seq(Sequence To Sequence)\n",
        "* 2014년 구글에서 논문으로 제안한 모델\n",
        "* LSTM(Long Short-Term Memory)또는 GRU(Gated Recurrent Unit)기반의 구조를 가지고 고정된 길이의 단어 시퀀스를 입력으로 받아, 입력 시퀀스에 알맞은 길이의 시퀀스를 출력해주는 언어 모델\n",
        "* 본 논문은 2개의 LSTM을 각각 Encoder와 Decoder로 사용해 가변적인 길이의 입/출력을 처리하고자 했음\n",
        "* [Seq2Seq](https://wikidocs.net/24996) 모델은 기계 번역 작업에서 큰 성능 향상을 가져왔고 , 특히 긴 문장을 처리하는데 강점이 있음"
      ],
      "metadata": {
        "id": "6xaaxQxOmpv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 과제\n",
        "- 규제(릿지, 라쏘) L1, L2 규제\n",
        "- 스태킹(Stacking)\n",
        "- 블렌딩(Weighted Blending)\n",
        "- 엘라스틱넷"
      ],
      "metadata": {
        "id": "KqmH0MuPqVXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. Seq2Seq 모델 구조\n",
        "* Seq2Seq는 한 문장을 다른 문장으로 변환하는 모델\n",
        "* 가변 길이의 입/출력을 처리하기 위해 인코더, 디코더 구조를 채택\n",
        "* 인코더와 디코더는 모두 여러 개의 LSTM 또는 GRU 셀로 구성되어 있음\n",
        "* 바닐라 RNN 대신 LSTM과 GRU 셀을 사용하는 이유는 LSTM과 GRU 모델이 RNN이 가지는 또 다른 한계점인 Long-trem dependency를 해결 하기 위해"
      ],
      "metadata": {
        "id": "BO6teqXUEwgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-3. 인코더\n",
        "* 입력 문장을 컨텍스트 벡터에 인코딩(압축)하는 역할을 함\n",
        "* 인코더의 LSTM은 입력 문장을 단어 순서대로 처리하여 고정된 크기의 컨텍스트 벡터를 반환\n",
        "* 컨텍스트 벡터는 인코더의 마지막 스텝에서 출력된 hidden state와 같음\n",
        "* 컨텍스트 벡터는 입력 문장의 정보를 함축하는 벡터이므로, 해당 멕터를 입력 문장에 대한 문장 수준의 벡터로 활용할 수 있음"
      ],
      "metadata": {
        "id": "SmelB7ro3xJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-4. 디코더\n",
        "* 입력 문장의 정보가 압축된 컨텍스트 벡터를 사용하여 출력 문장을 디코딩하는 역할\n",
        "* 디코더의 LSTM은 인코더로부터 전달받은 컨텍스트 벡터와 문장을 시작을 뜻하는 <sos>토큰을 입력으로 받아서, 문장의 끝을 뜻하는 <eos>토큰이 나올 때까지 문장을 생성\n",
        "* LSTM의 첫 셀에서는 <sos>토큰과 컨텍스트 벡터를 입력받아서 그 다음에 등장할 확률이 가장 높은 단어를 예측하고, 다음 스텝에서 예측한 단어를 입력으로 받아서, 그 다음에 등장할 확률이 가장 높은 단어를 예측함\n",
        "* 위 과정을 재귀적으로 반복하다가 다음에 등장할 확률이 가장 높은 단어로 <eos>토큰이 나오면 생성을 종료"
      ],
      "metadata": {
        "id": "2DDFKQea5HMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-5. 학습 과정\n",
        "* 모델을 학습할 때 Teacher Forcing이라는 기법을 사용\n",
        "* 모델 학습 과정에서는 이전 셀에서 예측한 단어를 다음 셀의 입력으로 넣어주는 대신 실제 정답 단어를 다음 셀의 입력으로 넣음\n",
        "* 만약 위 방법으로 학습하지 않으면 이전 셀에서의 오류가 자음 셀로 계속 전파될 것이고, 그러면 학습이 제대로 되지 않음"
      ],
      "metadata": {
        "id": "3Nql9iv66DuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-6. Seq2Seq 모델의 한계\n",
        "* 가변적인 길이의 입/출력을 처리하는데 효과적인 모델 구조이며, 실제로 기계 번역 작업에서 성능 향상을 거뒀으나 여전히 한계를 가짐\n",
        "* 인코더가 출력하는 벡터 사이즈가 고정되어 있기 때문에, 입력으로 들어오는 단어의 수가 매우 많아지면 성능이 떨어짐\n",
        "* RNN 구조의 한계점인 hidden state를 통해 이전 셀의 정보를 다음 셀로 계속 전달하는데 문장읠 길이가 길어지면 초기 셀에서 전달했던 정보들이 점차 흐려짐\n",
        "* LSTM, GRU 같은 모델들이 제안되긴 했으나 여전히 이전 정보를 계쏙 압축하는데 한계는 있음"
      ],
      "metadata": {
        "id": "15BW9QtB-6hn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Attention Mechanism\n",
        "* 기본적으로 Seq2Seq 모델의 한계를 해결하기 위해 2014년도에 제안한 논문\n",
        "* 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위해 등장한 기법"
      ],
      "metadata": {
        "id": "scESBF5V_4he"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-1. 어텐션의 아이디어\n",
        "* https://wikidocs.net/22893\n",
        "* 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점\n",
        "* 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야 할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중해서 조게 함"
      ],
      "metadata": {
        "id": "cq9w3XtFAeal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-2. 어텐션 함수\n",
        "* 어텐션을 함수로 표현\n",
        "    - Attention(Q, K, V) = Attention Value\n",
        "* 어텐션 함수는 주어진 쿼리에 대해서 모든 키와의 유사도를 각각 계산\n",
        "* 계산된 유사도를 키와 맵핑되어 있는 각각의 값에 반영한 뒤 유사도가 반영된 값을 모두 더해서 반환(어텐션 값)"
      ],
      "metadata": {
        "id": "xANxmKv-A4nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-3. 어텐션과 Seq2Seq\n",
        "* 어텐션 메커니즘은 Seq2Seq 모델이 가지는 한계를 해결하기 위해 제안 되었기 때문에 논문에서는 Seq2Seq 모델에 어텐션 메커니즘을 적용한 모델을 제안\n",
        "\n",
        "    - Q = Query : t 시점의 디코더 셀에서의 은닉 상태\n",
        "    - K = Keys : 모든 시점의 인코더 셀의 은닉 상태들\n",
        "    - V = Values : 모든 시점의 인코더 셀의 은닉 상태들"
      ],
      "metadata": {
        "id": "uIq04t0UCZyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-4. 어텐션 작동 원리\n",
        "* 디코더의 첫번째, 두번째 LSTM셀은 어텐션 메커니즘을 통해 단어를 예측하는 과정을 거쳤다고 가정하고 디코더의 세번째 LSTM 셀은 출력 단어를 예측하기 위해서 인코더의 모든 입력 단어들의 정보를 다시 참고\n",
        "* 인코더의 시점을 가각 1, 2, ..N이라고 했을 때 인코더의 각 셀에 대한 hidden state를 각각 h1, h2, .. hn이라고 하고, 디코더의 현재 시컴 t에서 해당 셀의 hidden state를 St\n",
        "* 어텐션 스코어는 hidden state 벡터 간 dot product를 사용하여 계산함\n",
        "\n"
      ],
      "metadata": {
        "id": "vSSLxMgLEjYe"
      }
    }
  ]
}