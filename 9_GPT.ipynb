{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNV7WOy0+WNjn9uZx4sB6ZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeCodingStudio/Python_NLP/blob/master/9_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. GPT(Generative Pre-Training)\n",
        "* GPT 모델은 2018년 6월에 OpenAI가 논문에서 처음 제안\n",
        "* GPT도 unlabeled data로 부터 pre-train을 진행한 후, 이를 특정 downstream task(with labeled data)에 fine-tuning(transfer learning)을 하는 모델\n",
        "* Transformer의 decoder만 사용하는 구조!"
      ],
      "metadata": {
        "id": "Xqhcs6y216Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. GPT 모델의 특징\n",
        "* 사전학습에는 대규모의 unlabeled data를 사용하는데 unlabeled data에서 단어 수준 이상의 정보를 얻는 것은 매우 힘듦 또한 어떤 방법이 유효한 텍스트 표현을 배우는데 효과적인지 불분명함\n",
        "* 사전학습 이후에도, 어떤 방법이 fine-tuning에 가장 효과적인지 불분명\n",
        "* 논문에서는 unsupervised pre-training과 supervised fine-tuning의 조합을 사용한 접근법을 제안\n",
        "* 목표는 fine-tuning에서의 미세 조정만으로 다양한 자연어처리 작업에 적용할 수 있는 범용적인 표현을 사전학습에서 학습하는 것\n",
        "* 모델은 이미 효과가 검증된 2017년 공개된 transformer를 사용"
      ],
      "metadata": {
        "id": "S590w2X83Kar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. GPT 모델 구조\n",
        "* GPT는 Transformer의 변형인 multi-layer Transformer decoder만 사용\n",
        "* 입력 문맥 token에 multi-headed self-attention을 적용한 후, 목표 token에 대한 출력 분포를 얻기 위해 position-wise feedforward layer를 적용"
      ],
      "metadata": {
        "id": "lARE7ndT4cFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. GPT 모델 학습\n",
        "* unsupervised pre-training: 대규모 코퍼스에서 unsupervised learning으로 언어 모델을 학습\n",
        "    - transformer 디코더를 사용하여 계속 next token prediction 학습하는 것\n",
        "    - multi-layer Transfoemer decoder를 사용하여 입력 문맥 token에 multi-headed self-attention을 적용한 후 목표 token에 대한 출력 분포를 얻기 위해 position-wise feedforward layer를 적용\n",
        "* supervised fine-tuning: 특정 작업에 대한 데이터로 모델을 fine-tuning\n",
        "    - 파인 튜닝 단계에서는 사전 학습된 모델을 각 태스크에 맞게 input과 label로 구성된 supervised dataset에 대해 학습\n",
        "    - GPT-1의 output을 sownstream 태스크에 적정하도록 선형변환한 후 softmax에 넣음\n",
        "    - 결과를 태스크에 맞는 loss들을 결합\n",
        "\n",
        "> 사전 학습은 next token prediction이라는 language modeling으로 진행되었기 때문에 각 downstream task와 input의 모양이 다를 수 밖에 없음"
      ],
      "metadata": {
        "id": "eVGL4jyr5VKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-4. GPT 모델의 의의\n",
        "* 모델 구조의 변형이 거의 없음\n",
        "    - 이전의 사전학습 모델들은 finetune시 모델 구조를 변형해야 하는 문제점이 있음\n",
        "    - 단, GPT-1은 GPT-1의 모델 구조를 전혀 안 바꾸고 입력만 바꿈\n",
        "* 추가되는 파라미터의 수가 매우 적음\n",
        "    - 모델 구조를 변형하지 않고, linear layer를 마지막에 추가하는 아주 간단한 추가작업만 수행\n",
        "    - finetuning 시점에서 random하게 초기화된 initial weight가 거의 linear layer뿐!"
      ],
      "metadata": {
        "id": "lVvRNlKa5sD3"
      }
    }
  ]
}