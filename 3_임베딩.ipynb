{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9pAv3CrSclxnpdH+qAVWz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeCodingStudio/Python_NLP/blob/master/3_%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 자연어 특성\n",
        "* 자연어를 기계가 처리하도록 하기 위해서는 먼저 자연어를 기계가 이해할 수 있는 언어로 바꾸는 방법을 알아야 함\n",
        "* 토큰화 작업의 결과인 단어 사전을 기계가 이해할 수 있는 언어로 표현하는 과정이고 단어 사전 내 단어 하나를 어떻게 표현할까의 문제로 볼 수 있음"
      ],
      "metadata": {
        "id": "XYOvJgEPuJVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 단어의 유사성과 모호성\n",
        "* 대부분의 언어에서 단어의 의미는 유사성과 모호성을 가지고 있는데 단어는 겉으로 보이는 형태인 표제어안에 여러가지 의미를 담고 있음\n",
        "* 대부분 사람의 주변 정보에 따라 숨겨진 의미를 파악하고 이해할 수 있으나 기계는 학습의 부재 또는 잘못된 데이터로 의미를 파악하지 못하는 경우가 있음\n",
        "* 한 가지 형태의 단어에 여러 의미가 포함되어 생기는 중의성 문제는 자연어 처리에서 매우 중요\n",
        "    - 동형어와 다의어\n",
        "        - 동형어: 형태는 같으나 뜻이 서로 다른 단어(배)\n",
        "        - 다의어: 하나의 형태가 여러 의미를 지니면서도 그 의미들이 서로 관련이 있는 단어(먹다)\n",
        "    - 동의어\n",
        "        - 서로 다른 형태의 단어들이 동일한 의미를 가지는 언어\n",
        "    - 상위어와 하위어\n",
        "        - 상위어: 상위 개념을 가리키는 단어\n",
        "        - 하위어: 하위 개념을 표현하는 단어"
      ],
      "metadata": {
        "id": "yXWDfyeKu2Yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. 언어의 모호성 해소\n",
        "* 동형어나 다의어 처럼 여러 의미를 가지는 단어들이 하나의 형태로 공유, 동의어처럼 하나의 형태를 가지는 단어들이 서로 같은 의미를 공유\n",
        "* 단어 중의성 해소(WSD) 알고리즘 방법을 통해 단어의 의미를 명확히 함\n",
        "    - 지식 기반 단어 중의성 해소\n",
        "        * 컴퓨터가 읽을 수 있는 사전이나 시소러스(어휘집) 등을 바탕으로 단어의 의미를 추론하는 접근 방식\n",
        "        * 사람이 직접 선별해서 데이터를 넣으므로 노이즈가 적음\n",
        "        * 영어 자연어처리 분야에서 가장 유명한 WordNet이 있음\n",
        "        * 관계 구축에 많은 리소스가 필요함\n",
        "        * 시의성을 반영하지 못함\n",
        "        * 데이터 편향이 생길 수 있음\n",
        "    - 지도 학습 기반 단어 중의성 해소\n",
        "        * 지도 학습은 데이터에 정답이 있다는 의미러, 각종 기계 학습 알고리즘을 통해 단어 의미를 분류해내는 방법\n",
        "        * WSD라고 하면 보통 단어의 세부 의미가 부착된 코퍼스를 학습 데이터로 사용하여 학습에 쓰이지 않았던 새로운 문장에서 단어 의미를 판별해내는 경우\n",
        "        * 좋은 성능을 위해서는 질 높은 레이블을 가진 크기가 큰 데이터가 필요함\n",
        "        * 데이터가 충분할 경우 일반화된 환경에서도 괜찮은 성능을 낼 수 있음\n",
        "    - 비지도 학습 기반 단어 중의성 해소\n",
        "        * 비지도 학습 WSD는 단어 의미 추론 작업인 WSI를 가리키는 경우가 많음\n",
        "        * 문장에 등장하는 각 단어의 의미를 사전적인 의미에 연결하지 않고, 세부 의미가 같은 맥락을 군집화하는 데에 초점을 맞춤\n",
        "        * 대규모 자연어 코퍼스로부터 추가 작업 없이 자동적으로 학습을 수행할 수 있어서 활용 가능성이 높음\n",
        "        * 사람이 직접 제작한 학습 데이터를 사용하지 않기 때문에 성능을 내기 어려움"
      ],
      "metadata": {
        "id": "PCLHBe_qwLU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 임베딩 구축 방법"
      ],
      "metadata": {
        "id": "1lyNyS4EJaw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. 임베딩이란\n",
        "* 자연어처리 작업에서 특징 추출을 통해 자연어를 수치화하는 과정이 필요하고, 이런 벡터화의 과정이자 결과\n",
        "* 토큰화 작업의 목표는 사실상 임베딩을 만들기 위한 단어 사전을 구축하는 것"
      ],
      "metadata": {
        "id": "0oN7veALKOvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2. 임베딩의 역할\n",
        "* 자연어의 의미적인 정보 함축\n",
        "    * 자연어의 중요한 특징들을 추출하여 벡터로 압축하는 과정\n",
        "    * 임베딩으로 표현된 문장은 실제 자연어의 주요 정보들을 포함하고 있음\n",
        "    * 벡터인 만큼 사칙 연산이 가능하여 단어 벡터간 덧셈/뺄셈을 통해 단어들 사이의 의미적 문법적 관계를 도출\n",
        "    * 임베딩의 품질을 평가하기 위해 사용되는 단어 유추 평가 예시(https://word2vec.kr/search/)\n",
        "* 자연어 간 유사도 계산\n",
        "    * 자연어를 벡터로 표현하면, 코사인 유사도를 활용하여 두 벡터 간 유사도를 계산할 수 있음\n",
        "    * 코사인 유사도는 -1이상 1이하의 값을 가지며 값이 1에 가까울수록 유사도가 높다고 판단함\n",
        "* 전이 학습\n",
        "    * 품질 좋은 임베딩을 사용하기 위해 이미 만들어진 다른 작업을 학습하기 위한 입력값으로 쓰인 것\n",
        "    * 이미 만들어진 임베딩을 다른 작업을 학습하기 위한 입력값으로 쓰임 -> 전이 학습\n",
        "    * 매번 새로운 것을 배울 때 scratch 부터 시작한다면 매 학습이 오래 걸림\n",
        "    * 파인 튜닝: 학습하는데 임베딩을 초기화하여 사용하면 새로운 작업을 학습함에도 빠르게 학습할 수 있고 성능도 좋아짐"
      ],
      "metadata": {
        "id": "8fK7RszJK5Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-3. 임베딩을 만들 때 쓰는 통계적인 가설\n",
        "* 문장을 구성하는데 어떤 단어가 얼마나 많이 쓰이는가?\n",
        "* 문장에 어떤 단어가 같이 쓰이는가?\n",
        "* 단어가 어떤 순서로 등장하는가?"
      ],
      "metadata": {
        "id": "xzlnztzQLDPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-4. 단어 출현 빈도에 기반한 임베딩 구축 방법\n",
        "* 원 핫 인코딩\n",
        "    - 자연어를 0과 1로 구별하겠다는 인코딩 방법\n",
        "    - 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 나머지 인덱스에는 0을 부여하는 벡터 표현 방식\n",
        "    - 희소 문제(sparsity problem)\n",
        "        * 예) 단어 사전의 크기가 10,000이라면, 총 10,00개 중 현재 단어를 표현하는 1개의 차원만 1이고, 나머지 9,999개의 차원은 0으로 표현\n",
        "        * 대부분의 값들이 0인 행렬을 회소행렬이라 하는데, 단어가 늘어날수록 행렬의 크기는 계속 증가하나 증가하는 크기에 비해 표현의 효율성은 떨어짐\n",
        "    - 단어의 유사도를 표현하지 못함\n",
        "* Bag of words\n",
        "    - 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현빈도에 집중하는 자연어 코퍼스의 데이터 수치화 방법\n",
        "    - 각 단어에 고유한 정수 인덱스를 부여하여 단어 집합을 생성하고, 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듦\n",
        "    - 단어 단위의 압축 방식이기 때문에 희소 문제와 간어 순서를 반영하지 못함\n",
        "* TF-IDF\n",
        "    - 단어의 빈도와 역문서 빈도를 사용하여 문서-단어 행렬 내 각 단어들의 중요한 정도를 가중치로 주는 표현 방법\n",
        "    - 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에서 효과적으로 쓰일 수 있음\n",
        "    - 단어의 중요도를 고려해도 여전히 단어의 빈도로 판단하는 표현 방식이기 때문에 맥락적 유사도를 반영하지 못함\n",
        "    - 단어 사전의 규모 자체를 축소하는 방법이 아니기 때문에 단어 사전의 규모가 크다면 높은 계산 복잡도를 요구하는 표현 방식(희소 문제)"
      ],
      "metadata": {
        "id": "XkSw3_3PTBV8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymp4mEggTplZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}